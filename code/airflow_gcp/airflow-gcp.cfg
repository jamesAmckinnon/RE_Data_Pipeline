[core]
# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository. This path must be absolute.
#
# Variable: AIRFLOW__CORE__DAGS_FOLDER
#
dags_folder = /opt/airflow/dags

# Hostname by providing a path to a callable, which will resolve the hostname.
hostname_callable = airflow.utils.net.getfqdn

# A callable to check if a python file has airflow dags defined or not and should
# return ``True`` if it has dags otherwise ``False``.
might_contain_dag_callable = airflow.utils.file.might_contain_dag_via_default_heuristic

# Default timezone in case supplied date times are naive
default_timezone = utc

# The executor class that airflow should use.
executor = LocalExecutor

# The auth manager class that airflow should use.
auth_manager = airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager

# This defines the maximum number of task instances that can run concurrently per scheduler in
# Airflow, regardless of the worker count.
parallelism = 32

# The maximum number of task instances allowed to run concurrently in each DAG.
max_active_tasks_per_dag = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# The maximum number of active DAG runs per DAG.
max_active_runs_per_dag = 16

# The maximum number of consecutive DAG failures before DAG is automatically paused.
max_consecutive_failed_dag_runs_per_dag = 0

# Whether to load the DAG examples that ship with Airflow.
load_examples = False

# Path to the folder containing Airflow plugins
plugins_folder = /opt/airflow/plugins

# Should tasks be executed via forking of the parent process
execute_tasks_new_python_interpreter = False

# Secret key to save connection passwords in the db
fernet_key = 

# Whether to disable pickling dags
donot_pickle = True

# How long before timing out a python file import
dagbag_import_timeout = 30.0

# Should a traceback be shown in the UI for dagbag import errors
dagbag_import_error_tracebacks = True

# If tracebacks are shown, how many entries from the traceback should be shown
dagbag_import_error_traceback_depth = 2

# How long before timing out a DagFileProcessor
dag_file_processor_timeout = 50

# The class to use for running task instances in a subprocess.
task_runner = StandardTaskRunner

# If set, tasks without a ``run_as_user`` argument will be run with this user
default_impersonation = 

# What security module to use (for example kerberos)
security = 

# Turn unit test mode on
unit_test_mode = False

# Whether to enable pickling for xcom
enable_xcom_pickling = False

# What classes can be imported during deserialization.
allowed_deserialization_classes = airflow.*

# What classes can be imported during deserialization.
allowed_deserialization_classes_regexp = 

# When a task is killed forcefully, this is the amount of time in seconds that
# it has to cleanup after it is sent a SIGTERM, before it is SIGKILLED
killed_task_cleanup_time = 60

# Whether to override params with dag_run.conf
dag_run_conf_overrides_params = True

# If enabled, Airflow will only scan files containing both ``DAG`` and ``airflow``
dag_discovery_safe_mode = True

# The pattern syntax used in the .airflowignore files
dag_ignore_file_syntax = regexp

# The number of retries each task is going to have by default.
default_task_retries = 0

# The number of seconds each task is going to wait by default between retries.
default_task_retry_delay = 300

# The maximum delay (in seconds) each task is going to wait by default between retries.
max_task_retry_delay = 86400

# The weighting method used for the effective total priority weight of the task
default_task_weight_rule = downstream

# Maximum possible time (in seconds) that task will have for execution of auxiliary processes
task_success_overtime = 20

# The default task execution_timeout value for the operators.
default_task_execution_timeout = 

# Updating serialized DAG can not be faster than a minimum interval to reduce database write rate.
min_serialized_dag_update_interval = 30

# If ``True``, serialized DAGs are compressed before writing to DB.
compress_serialized_dags = False

# Fetching serialized DAG can not be faster than a minimum interval to reduce database
# read rate. This config controls when your DAGs are updated in the Webserver
min_serialized_dag_fetch_interval = 10

# Maximum number of Rendered Task Instance Fields (Template Fields) per task to store
max_num_rendered_ti_fields_per_task = 30

# On each dagrun check against defined SLAs
check_slas = True

# Path to custom XCom class that will be used to store and resolve operators results
xcom_backend = airflow.models.xcom.BaseXCom

# By default Airflow plugins are lazily-loaded
lazy_load_plugins = True

# By default Airflow providers are lazily-discovered
lazy_discover_providers = True

# Hide sensitive **Variables** or **Connection extra json keys** from UI
hide_sensitive_var_conn_fields = True

# A comma-separated list of extra sensitive keywords to look for in variables names
sensitive_var_conn_names = 

# Task Slot counts for ``default_pool``
default_pool_task_slot_count = 128

# The maximum list/dict length an XCom can push to trigger task mapping
max_map_length = 1024

# The default umask to use for process when run in daemon mode
daemon_umask = 0o077

# Class to use as dataset manager.
# dataset_manager_class = 

# Kwargs to supply to dataset manager.
# dataset_manager_kwargs = 

# Dataset URI validation should raise an exception if it is not compliant with AIP-60.
strict_dataset_uri_validation = False

# Whether components should use Airflow Internal API for DB connectivity.
database_access_isolation = False

# Airflow Internal API url.
# internal_api_url = 

# Secret key used to authenticate internal API clients to core.
internal_api_secret_key = 

# The ability to allow testing connections across Airflow UI, API and CLI.
test_connection = Disabled

# The maximum length of the rendered template field.
max_templated_field_length = 4096

[database]
# Path to the ``alembic.ini`` file.
alembic_ini_file_path = alembic.ini

# The SQLAlchemy connection string to the metadata database.
# This will be set via environment variable in GCP
sql_alchemy_conn = 

# Extra engine specific keyword args passed to SQLAlchemy's create_engine
# sql_alchemy_engine_args = 

# The encoding for the databases
sql_engine_encoding = utf-8

# Collation for ``dag_id``, ``task_id``, ``key``, ``external_executor_id`` columns
# sql_engine_collation_for_ids = 

# If SQLAlchemy should pool database connections.
sql_alchemy_pool_enabled = True

# The SQLAlchemy pool size is the maximum number of database connections
sql_alchemy_pool_size = 5

# The maximum overflow size of the pool.
sql_alchemy_max_overflow = 10

# The SQLAlchemy pool recycle is the number of seconds a connection
sql_alchemy_pool_recycle = 1800

# Check connection at the start of each connection pool checkout.
sql_alchemy_pool_pre_ping = True

# The schema to use for the metadata database.
sql_alchemy_schema = 

# Import path for connect args in SQLAlchemy.
# sql_alchemy_connect_args = 

# Import path for function which returns 'sqlalchemy.orm.sessionmaker'.
# sql_alchemy_session_maker = 

# Whether to load the default connections that ship with Airflow
load_default_connections = True

# Number of times the code should be retried in case of DB Operational Errors.
max_db_retries = 3

# Whether to run alembic migrations during Airflow start up.
check_migrations = True

[logging]
# The folder where airflow should store its log files.
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
remote_logging = True

# Users must supply an Airflow connection id that provides access to the storage
remote_log_conn_id = gcs_default

# Whether the local log files for GCS, S3, WASB and OSS remote logging should be deleted
delete_local_logs = True

# Path to Google Credential JSON file.
google_key_path = 

# Storage bucket URL for remote logging
remote_base_log_folder = 

# The remote_task_handler_kwargs param is loaded into a dictionary
remote_task_handler_kwargs = 

# Use server-side encryption for logs stored in S3
encrypt_s3_logs = False

# Logging level.
logging_level = INFO

# Logging level for celery.
celery_logging_level = 

# Logging level for Flask-appbuilder UI.
fab_logging_level = WARNING

# Logging class
logging_config_class = 

# Flag to enable/disable Colored logs in Console
colored_console_log = True

# Log format for when Colored logs is enabled
colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s

# Specifies the class utilized by Airflow to implement colored logging
colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter

# Format of Log line
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s

# Defines the format of log messages for simple logging configuration
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

# Where to send dag parser logs.
dag_processor_log_target = file

# Format of Dag Processor Log line
dag_processor_log_format = [%%(asctime)s] [SOURCE:DAG_PROCESSOR] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s

# Determines the formatter class used by Airflow for structuring its log messages
log_formatter_class = airflow.utils.log.timezone_aware.TimezoneAware

# An import path to a function to add adaptations of each secret added
secret_mask_adapter = 

# Specify prefix pattern like mentioned below with stream handler
task_log_prefix_template = 

# Formatting for how airflow generates file names/paths for each task run.
log_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{%% if ti.map_index >= 0 %%}map_index={{ ti.map_index }}/{%% endif %%}attempt={{ try_number }}.log

# Formatting for how airflow generates file names for log
log_processor_filename_template = {{ filename }}.log

# Full path of dag_processor_manager logfile.
dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log

# Whether DAG processor manager will write logs to stdout
dag_processor_manager_log_stdout = False

# Name of handler to read task instance logs.
task_log_reader = task

# A comma\-separated list of third-party logger names
extra_logger_names = 

# When you start an Airflow worker, Airflow starts a tiny web server
worker_log_server_port = 8793

# Port to serve logs from for triggerer.
trigger_log_server_port = 8794

# Permissions in the form or of octal string as understood by chmod.
file_task_handler_new_folder_permissions = 0o775

# Permissions in the form or of octal string as understood by chmod.
file_task_handler_new_file_permissions = 0o664

# By default Celery sends all logs into stderr.
celery_stdout_stderr_separation = False

# If enabled, Airflow may ship messages to task logs from outside the task run context
enable_task_context_logger = True

# A comma separated list of keywords related to errors
color_log_error_keywords = error,exception

# A comma separated list of keywords related to warning
color_log_warning_keywords = warn

[webserver]
# The message displayed when a user attempts to execute actions beyond their authorised privileges.
access_denied_message = Access is Denied

# Path of webserver config file used for configuring the webserver parameters
config_file = /opt/airflow/config/webserver_config.py

# The base url of your website
base_url = http://localhost:8080

# Default timezone to display all dates in the UI
default_ui_timezone = UTC

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server.
web_server_ssl_cert = 

# Paths to the SSL certificate and key for the web server.
web_server_ssl_key = 

# The type of backend used to store web session data
session_backend = database

# Number of seconds the webserver waits before killing gunicorn master
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to refresh at a time.
worker_refresh_batch_size = 1

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 6000

# If set to ``True``, Airflow will track files in plugins_folder directory.
reload_on_plugin_change = False

# Secret key used to run your flask app.
secret_key = 

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use.
worker_class = sync

# Log files for the gunicorn webserver.
access_logfile = -

# Log files for the gunicorn webserver.
error_logfile = -

# Access log format for gunicorn webserver.
access_logformat = 

# Expose the configuration file in the web server.
expose_config = False

# Expose hostname in the web server
expose_hostname = False

# Expose stacktrace in the web server
expose_stacktrace = False

# Default DAG view.
dag_default_view = grid

# Default DAG orientation.
dag_orientation = LR

# Sorting order in grid view.
grid_view_sorting_order = topological

# The amount of time (in secs) webserver will wait for initial handshake
log_fetch_timeout_sec = 5

# Time interval (in secs) to wait before next log fetching.
log_fetch_delay_sec = 2

# Distance away from page bottom to enable auto tailing.
log_auto_tailing_offset = 30

# Animation speed for auto tailing log display.
log_animation_speed = 1000

# By default, the webserver shows paused DAGs.
hide_paused_dags_by_default = False

# Consistent page size across all listing views in the UI
page_size = 100

# Define the color of navigation bar
navbar_color = #fff

# Define the color of text in the navigation bar
navbar_text_color = #51504f

# Define the color of navigation bar links when hovered
navbar_hover_color = #eee

# Define the color of text in the navigation bar when hovered
navbar_text_hover_color = #51504f

# Define the color of the logo text
navbar_logo_text_color = #51504f

# Default dagrun to show in UI
default_dag_run_display_number = 25

# Enable werkzeug ``ProxyFix`` middleware for reverse proxy
enable_proxy_fix = False

# Number of values to trust for ``X-Forwarded-For``.
proxy_fix_x_for = 1

# Number of values to trust for ``X-Forwarded-Proto``.
proxy_fix_x_proto = 1

# Number of values to trust for ``X-Forwarded-Host``.
proxy_fix_x_host = 1

# Number of values to trust for ``X-Forwarded-Port``.
proxy_fix_x_port = 1

# Number of values to trust for ``X-Forwarded-Prefix``.
proxy_fix_x_prefix = 1

# Set secure flag on session cookie
cookie_secure = False

# Set samesite policy on session cookie
cookie_samesite = Lax

# Default setting for wrap toggle on DAG code and TI log views.
default_wrap = False

# Allow the UI to be rendered in a frame
x_frame_enabled = True

# 'Recent Tasks' stats will show for old DagRuns if set
show_recent_stats_for_completed_runs = True

# The UI cookie lifetime in minutes.
session_lifetime_minutes = 43200

# Whether the custom page title for the DAGs overview page contains any Markup language
instance_name_has_markup = False

# How frequently, in seconds, the DAG data will auto-refresh in graph or grid view
auto_refresh_interval = 3

# Boolean for displaying warning for publicly viewable deployment
warn_deployment_exposure = True

# Boolean for running SwaggerUI in the webserver.
enable_swagger_ui = True

# Boolean for running Internal API in the webserver.
run_internal_api = False

# The caching algorithm used by the webserver.
caching_hash_method = md5

# Behavior of the trigger DAG run button for DAGs without params.
show_trigger_form_if_no_params = False

# Number of recent DAG run configurations in the selector on the trigger web form.
num_recent_configurations_for_trigger = 5

# A DAG author is able to provide any raw HTML into ``doc_md`` or params description
allow_raw_html_descriptions = False

# The maximum size of the request payload (in MB) that can be sent.
allowed_payload_size = 1.0

# Require confirmation when changing a DAG in the web UI.
require_confirmation_dag_change = False

# The maximum size in bytes any non-file form field may be in a multipart/form-data body.
max_form_memory_size = 500000

# The maximum number of fields that may be present in a multipart/form-data body.
max_form_parts = 1000

[scheduler]
# Task instances listen for external kill signal
job_heartbeat_sec = 5

# The scheduler constantly tries to trigger new tasks
scheduler_heartbeat_sec = 5

# The frequency (in seconds) at which the LocalTaskJob should send heartbeat signals
local_task_job_heartbeat_sec = 0

# The number of times to try to schedule each DAG file
num_runs = -1

# Controls how long the scheduler will sleep between loops
scheduler_idle_sleep_time = 1

# Number of seconds after which a DAG file is parsed.
min_file_process_interval = 30

# How often (in seconds) to check for stale DAGs
parsing_cleanup_interval = 60

# How long (in seconds) to wait after we have re-parsed a DAG file before deactivating stale DAGs
stale_dag_threshold = 50

# How often (in seconds) to scan the DAGs directory for new files.
dag_dir_list_interval = 300

# How often should stats be printed to the logs.
print_stats_interval = 30

# How often (in seconds) should pool usage stats be sent to StatsD
pool_metrics_interval = 5.0

# If the last scheduler heartbeat happened more than this threshold ago
scheduler_health_check_threshold = 30

# When you start a scheduler, airflow starts a tiny web server
enable_health_check = True

# When you start a scheduler, airflow starts a tiny web server
scheduler_health_check_server_host = 0.0.0.0

# When you start a scheduler, airflow starts a tiny web server
scheduler_health_check_server_port = 8974

# How often (in seconds) should the scheduler check for orphaned tasks and SchedulerJobs
orphaned_tasks_check_interval = 300.0

# Determines the directory where logs for the child processes of the scheduler will be stored
child_process_log_directory = /opt/airflow/logs/scheduler

# Local task jobs periodically heartbeat to the DB.
scheduler_zombie_task_threshold = 300

# How often (in seconds) should the scheduler check for zombie tasks.
zombie_detection_interval = 10.0

# Turn off scheduler catchup by setting this to ``False``.
catchup_by_default = False

# Setting this to ``True`` will make first task instance of a task
ignore_first_depends_on_past_by_default = True

# This changes the batch size of queries in the scheduling main loop.
max_tis_per_query = 16

# Should the scheduler issue ``SELECT ... FOR UPDATE`` in relevant queries.
use_row_level_locking = True

# Max number of DAGs to create DagRuns for per scheduler loop.
max_dagruns_to_create_per_loop = 10

# How many DagRuns should a scheduler examine (and lock) when scheduling
max_dagruns_per_loop_to_schedule = 20

# Should the Task supervisor process perform a "mini scheduler"
schedule_after_task_execution = True

# The scheduler reads dag files to extract the airflow modules
parsing_pre_import_modules = True

# The scheduler can run multiple processes in parallel to parse dags.
parsing_processes = 2

# One of ``modified_time``, ``random_seeded_by_host`` and ``alphabetical``.
file_parsing_sort_mode = modified_time

# Whether the dag processor is running as a standalone process
standalone_dag_processor = False

# Only applicable if standalone_dag_processor is true
max_callbacks_per_loop = 20

# Only applicable if standalone_dag_processor is true.
dag_stale_not_seen_duration = 600

# Turn off scheduler use of cron intervals by setting this to ``False``.
use_job_schedule = True

# Allow externally triggered DagRuns for Execution Dates in the future
allow_trigger_in_future = False

# How often to check for expired trigger requests that have not run yet.
trigger_timeout_check_interval = 15

# Amount of time a task can be in the queued state before being retried or set to failed.
task_queued_timeout = 600.0

# How often to check for tasks that have been in the queued state
task_queued_timeout_check_interval = 120.0

# The run_id pattern used to verify the validity of user input
allowed_run_id_pattern = ^[A-Za-z0-9_.~:+-]+$

# Whether to create DAG runs that span an interval or one single point in time for cron schedules
create_cron_data_intervals = True

[triggerer]
# How many triggers a single Triggerer will run at once, by default.
default_capacity = 1000

# How often to heartbeat the Triggerer job to ensure it hasn't been killed.
job_heartbeat_sec = 5

# If the last triggerer heartbeat happened more than this threshold ago
triggerer_health_check_threshold = 30

[operators]
# The default owner assigned to each new operator
default_owner = airflow

# The default value of attribute "deferrable" in operators and sensors.
default_deferrable = false

# Indicates the default number of CPU units allocated to each operator
default_cpus = 1

# Indicates the default number of RAM allocated to each operator
default_ram = 512

# Indicates the default number of disk storage allocated to each operator
default_disk = 512

# Indicates the default number of GPUs allocated to each operator
default_gpus = 0

# Default queue that tasks get assigned to and that worker listen on.
default_queue = default

# Is allowed to pass additional/unused arguments (args, kwargs) to the BaseOperator operator.
allow_illegal_arguments = False

[api]
# Enables the deprecated experimental API.
enable_experimental_api = False

# Comma separated list of auth backends to authenticate users of the API.
auth_backends = airflow.api.auth.backend.session

# Used to set the maximum page limit for API requests.
maximum_page_limit = 100

# Used to set the default page limit when limit param is zero or not provided in API
fallback_page_limit = 100

# The intended audience for JWT token credentials used for authorization.
google_oauth2_audience = 

# Path to Google Cloud Service Account key file (JSON).
google_key_path = 

# Used in response to a preflight request to indicate which HTTP headers can be used
access_control_allow_headers = 

# Specifies the method or methods allowed when accessing the resource.
access_control_allow_methods = 

# Indicates whether the response can be shared with requesting code from the given origins.
access_control_allow_origins = 

# Indicates whether the **xcomEntries** endpoint supports the **deserialize** flag.
enable_xcom_deserialize_support = False

[lineage]
# what lineage backend to use
backend = 

[email]
# Email backend to use
email_backend = airflow.utils.email.send_email_smtp

# Email connection to use
email_conn_id = smtp_default

# Whether email alerts should be sent when a task is retried
default_email_on_retry = True

# Whether email alerts should be sent when a task failed
default_email_on_failure = True

# File that will be used as the template for Email subject
# subject_template = 

# File that will be used as the template for Email content
# html_content_template = 

# Email address that will be used as sender address.
# from_email = 

# ssl context to use when using SMTP and IMAP SSL connections.
ssl_context = default

[smtp]
# Specifies the host server address used by Airflow when sending out email notifications via SMTP.
smtp_host = localhost

# Determines whether to use the STARTTLS command when connecting to the SMTP server.
smtp_starttls = True

# Determines whether to use an SSL connection when talking to the SMTP server.
smtp_ssl = False

# Username to authenticate when connecting to smtp server.
# smtp_user = 

# Password to authenticate when connecting to smtp server.
# smtp_password = 

# Defines the port number on which Airflow connects to the SMTP server
smtp_port = 25

# Specifies the default **from** email address used when Airflow sends email notifications.
smtp_mail_from = airflow@example.com

# Determines the maximum time (in seconds) the Apache Airflow system will wait
smtp_timeout = 30

# Defines the maximum number of times Airflow will attempt to connect to the SMTP server.
smtp_retry_limit = 5

[sentry]
# Enable error reporting to Sentry
sentry_on = false

# Sentry DSN
sentry_dsn = 

# Dotted path to a before_send function that the sentry SDK should be configured to use.
# before_send = 

[debug]
# Used only with ``DebugExecutor``.
fail_fast = False

[metrics]
# If true, metrics_allow_list and metrics_block_list will use regex pattern matching
metrics_use_pattern_match = False

# Configure an allow list (comma separated string) to send only certain metrics.
metrics_allow_list = 

# Configure a block list (comma separated string) to block certain metrics
metrics_block_list = 

# Enables sending metrics to StatsD.
statsd_on = False

# Specifies the host address where the StatsD daemon (or server) is running
statsd_host = localhost

# Specifies the port on which the StatsD daemon (or server) is listening to
statsd_port = 8125

# Defines the namespace for all metrics sent from Airflow to StatsD
statsd_prefix = airflow

# A function that validate the StatsD stat name
stat_name_handler = 

# To enable datadog integration to send airflow metrics.
statsd_datadog_enabled = False

# List of datadog tags attached to all metrics
statsd_datadog_tags = 

# Set to ``False`` to disable metadata tags for some of the emitted metrics
statsd_datadog_metrics_tags = True

# If you want to utilise your own custom StatsD client set the relevant module path below.
# statsd_custom_client_path = 

# If you want to avoid sending all the available metrics tags to StatsD
statsd_disabled_tags = job_id,run_id

# To enable sending Airflow metrics with StatsD-Influxdb tagging convention.
statsd_influxdb_enabled = False

# Enables sending metrics to OpenTelemetry.
otel_on = False

# Specifies the hostname or IP address of the OpenTelemetry Collector
otel_host = localhost

# Specifies the port of the OpenTelemetry Collector that is listening to.
otel_port = 8889

# The prefix for the Airflow metrics.
otel_prefix = airflow

# Defines the interval, in milliseconds, at which Airflow sends batches of metrics
otel_interval_milliseconds = 60000

# If ``True``, all metrics are also emitted to the console.
otel_debugging_on = False

# The default service name of traces.
otel_service = Airflow

# If ``True``, SSL will be enabled.
otel_ssl_active = False

[traces]
# Enables sending traces to OpenTelemetry.
otel_on = False

# Specifies the hostname or IP address of the OpenTelemetry Collector
otel_host = localhost

# Specifies the port of the OpenTelemetry Collector that is listening to.
otel_port = 8889

# The default service name of traces.
otel_service = Airflow

# If True, all traces are also emitted to the console.
otel_debugging_on = False

# If True, SSL will be enabled.
otel_ssl_active = False

# If True, after the task is complete, the full task log messages will be added
otel_task_log_event = False

[secrets]
# Full class name of secrets backend to enable
backend = 

# The backend_kwargs param is loaded into a dictionary and passed to ``__init__``
backend_kwargs = 

# Enables local caching of Variables, when parsing DAGs only.
use_cache = False

# When the cache is enabled, this is the duration for which we consider an entry in the cache
cache_ttl_seconds = 900

[cli]
# In what way should the cli access the API.
api_client = airflow.api.client.local_client

# If you set web_server_url_prefix, do NOT forget to append it here
endpoint_url = http://localhost:8080

[kerberos]
# Location of your ccache file once kinit has been performed.
ccache = /tmp/airflow_krb5_ccache

# gets augmented with fqdn
principal = airflow

# Determines the frequency at which initialization or re-initialization processes occur.
reinit_frequency = 3600

# Path to the kinit executable
kinit_path = kinit

# Designates the path to the Kerberos keytab file for the Airflow user
keytab = airflow.keytab

# Allow to disable ticket forwardability.
forwardable = True

# Allow to remove source IP from token, useful when using token behind NATted Docker host.
include_ip = True

[sensors]
# Sensor default timeout, 7 days by default (7 * 24 * 60 * 60).
default_timeout = 604800

[common.io]
# Path to a location on object storage where XComs can be stored in url format.
xcom_objectstorage_path = 

# Threshold in bytes for storing XComs in object storage.
xcom_objectstorage_threshold = -1

# Compression algorithm to use when storing XComs in object storage.
xcom_objectstorage_compression = 

[fab]
# Boolean for enabling rate limiting on authentication endpoints.
auth_rate_limited = True

# Rate limit for authentication endpoints.
auth_rate_limit = 5 per 40 second

# Update FAB permissions and sync security manager roles on webserver startup
update_fab_perms = True

[imap]
# Options for IMAP provider.
# ssl_context = 

[smtp_provider]
# Options for SMTP provider.
# ssl_context = 

# Allows overriding of the standard templated email subject line
# templated_email_subject_path = 

# Allows overriding of the standard templated email path
# templated_html_content_path = 
